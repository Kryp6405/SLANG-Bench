{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34edbc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1d946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize embedding and language models\n",
    "embedding_model = 'snowflake-arctic-embed'\n",
    "language_model = 'llama3.1:8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b45164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_excel('../combined_slang_data_multi.xlsx')\n",
    "\n",
    "# create a string per unique word by combining all rows for that word\n",
    "combined_rows_as_strings_dict = {}\n",
    "for i, row in df.iterrows():\n",
    "    # append or create string entry for each word\n",
    "    word = row['slang']\n",
    "    row_string = \"\\n\".join(f\"{k}: {v}\" for k, v in row.items())\n",
    "    combined_rows_as_strings_dict[word] = combined_rows_as_strings_dict.get(word, \"\") + \"\\n\" + row_string\n",
    "combined_rows_as_strings = list(combined_rows_as_strings_dict.values())\n",
    "\n",
    "VECTOR_DB = [] # a list of (text, embedding) tuples\n",
    "\n",
    "def add_text_to_db(text):\n",
    "    embedding = ollama.embed(model=embedding_model, input=text)['embeddings'][0]\n",
    "    VECTOR_DB.append((text, embedding))\n",
    "\n",
    "for row_text in combined_rows_as_strings:\n",
    "    add_text_to_db(row_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95206ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_json('eval_reverse.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7443e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    # simple tokenizer; you can make this smarter if you want\n",
    "    return text.lower().split()\n",
    "\n",
    "# texts in the same order as VECTOR_DB\n",
    "doc_texts = [text for text, _ in VECTOR_DB]\n",
    "tokenized_docs = [tokenize(t) for t in doc_texts]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e01df43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MATRIX = np.vstack([np.asarray(emb, dtype=np.float32) for _, emb in VECTOR_DB])\n",
    "EMB_NORMS = np.linalg.norm(EMB_MATRIX, axis=1) + 1e-9\n",
    "DOC_TEXTS = [text for text, _ in VECTOR_DB]  # keep alignment\n",
    "\n",
    "def hybrid_search(query_text, top_k=10, alpha=0.4):\n",
    "    q_embedding = np.asarray(\n",
    "        ollama.embed(model=embedding_model, input=query_text)['embeddings'][0],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    q_norm = np.linalg.norm(q_embedding) + 1e-9\n",
    "\n",
    "    # vectorized dense scores\n",
    "    dense_scores = (EMB_MATRIX @ q_embedding) / (EMB_NORMS * q_norm)\n",
    "\n",
    "    # BM25\n",
    "    query_tokens = tokenize(query_text)\n",
    "    bm25_scores = np.array(bm25.get_scores(query_tokens), dtype=np.float32)\n",
    "\n",
    "    # min-max normalize\n",
    "    def minmax_normalize(x):\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        if x_max == x_min:\n",
    "            return np.zeros_like(x)\n",
    "        return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    dense_norm = minmax_normalize(dense_scores)\n",
    "    bm25_norm = minmax_normalize(bm25_scores)\n",
    "\n",
    "    hybrid_scores = alpha * dense_norm + (1 - alpha) * bm25_norm\n",
    "\n",
    "    # fast top-k with argpartition\n",
    "    top_idx = np.argpartition(-hybrid_scores, top_k - 1)[:top_k]\n",
    "    # sort those top-k\n",
    "    top_idx = top_idx[np.argsort(-hybrid_scores[top_idx])]\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(top_idx, start=1):\n",
    "        text = DOC_TEXTS[idx]\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"index\": int(idx),\n",
    "            \"hybrid_score\": float(hybrid_scores[idx]),\n",
    "            \"dense_score\": float(dense_scores[idx]),\n",
    "            \"bm25_score\": float(bm25_scores[idx]),\n",
    "            \"text\": text,\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a37195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== f1 score ====================\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "_token_re = re.compile(r\"\\w+\")\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    return _token_re.findall(text.lower())\n",
    "\n",
    "# calculates f1 score between prediction and target\n",
    "def explanation_token_f1(pred: str, target: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple token-level F1 between prediction and target.\n",
    "    \"\"\"\n",
    "    pred_tokens = _tokenize(pred)\n",
    "    tgt_tokens  = _tokenize(target)\n",
    "\n",
    "    if not pred_tokens or not tgt_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    pred_counts: Dict[str, int] = {}\n",
    "    tgt_counts: Dict[str, int] = {}\n",
    "    for t in pred_tokens:\n",
    "        pred_counts[t] = pred_counts.get(t, 0) + 1\n",
    "    for t in tgt_tokens:\n",
    "        tgt_counts[t] = tgt_counts.get(t, 0) + 1\n",
    "\n",
    "    overlap = 0\n",
    "    for t, c in pred_counts.items():\n",
    "        if t in tgt_counts:\n",
    "            overlap += min(c, tgt_counts[t])\n",
    "\n",
    "    precision = overlap / max(len(pred_tokens), 1)\n",
    "    recall    = overlap / max(len(tgt_tokens), 1)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "#==================== exact match ====================\n",
    "\n",
    "def exact_match(pred: str, target: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if prediction exactly matches target (case-insensitive, whitespace-normalized).\n",
    "    \"\"\"\n",
    "    return pred.strip().lower() == target.strip().lower()\n",
    "\n",
    "#==================== average word length ====================\n",
    "\n",
    "def avg_word_length(texts: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute average word count across all texts.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return 0.0\n",
    "    total_words = sum(len(_tokenize(text)) for text in texts)\n",
    "    return total_words / len(texts)\n",
    "\n",
    "#==================== bertscore ====================\n",
    "\n",
    "# compute bertscore between prediction and target lists\n",
    "def compute_bertscore_f1(preds: List[str], targets: List[str]) -> float | None:\n",
    "    \"\"\"\n",
    "    Compute mean BERTScore F1 over (pred, target) pairs.\n",
    "    Requires `bert_score` package. If not available, returns None.\n",
    "    \"\"\"\n",
    "    if not preds or not targets:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        from bert_score import score as bertscore\n",
    "    except ImportError:\n",
    "        # You can pip install bert_score to enable this\n",
    "        print(\"[warn] bert_score not installed; skipping BERTScore metric.\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1 = bertscore(\n",
    "        preds,\n",
    "        targets,\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True,  # common setting\n",
    "    )\n",
    "    return float(F1.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5714c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(pre_rag_prompt, context):\n",
    "    prompt = f\"\\nContext:\\n{context}\\n{pre_rag_prompt}\"\n",
    "    return prompt\n",
    "\n",
    "def build_rag_context(prompt, top_k=5):\n",
    "    results = hybrid_search(prompt, top_k=top_k)\n",
    "    context = \"\\n\".join([r['text'] for r in results])\n",
    "    return context\n",
    "\n",
    "def evaluate_rag_model(df_eval, top_k=5):\n",
    "    token_f1_scores = []\n",
    "    exact_matches = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    prompts = []\n",
    "    responses = []\n",
    "    \n",
    "    num_questions = len(df_eval)\n",
    "    for i in range(len(df_eval)):\n",
    "        row = df_eval.iloc[i]\n",
    "        pre_rag_prompt = row['prompt']\n",
    "        context = build_rag_context(pre_rag_prompt, top_k=top_k)\n",
    "\n",
    "        rag_prompt = build_rag_prompt(pre_rag_prompt, context)\n",
    "        response = ollama.chat(\n",
    "            model=language_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains the meanings of sentences containing slang in standard English.\"},\n",
    "                {\"role\": \"user\", \"content\": rag_prompt},\n",
    "            ]\n",
    "        )\n",
    "        response_text = response.message.content\n",
    "\n",
    "        # handle potential None response\n",
    "        if response_text is None:\n",
    "            response_text = \"\"\n",
    "        \n",
    "        prompts.append(rag_prompt)\n",
    "        responses.append(response_text)\n",
    "        \n",
    "        target_response = row['target']\n",
    "\n",
    "        # calculate metrics\n",
    "        f1 = explanation_token_f1(response_text, target_response)\n",
    "        token_f1_scores.append(f1)\n",
    "        exact_matches.append(exact_match(response_text, target_response))\n",
    "        preds.append(response_text)\n",
    "        targets.append(target_response)\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(i)\n",
    "\n",
    "    # compute final metrics\n",
    "    mean_f1 = float(np.mean(token_f1_scores)) if token_f1_scores else 0.0\n",
    "    exact_match_rate = float(np.mean(exact_matches)) if exact_matches else 0.0\n",
    "    avg_pred_len = avg_word_length(preds)\n",
    "    mean_bertscore_f1 = compute_bertscore_f1(preds, targets)\n",
    "    \n",
    "    metrics = {\n",
    "        \"mean_f1\": mean_f1,\n",
    "        \"mean_bertscore_f1\": mean_bertscore_f1,\n",
    "        \"exact_match_rate\": exact_match_rate,\n",
    "        \"avg_pred_length_words\": avg_pred_len,\n",
    "        \"num_examples\": num_questions,\n",
    "    }\n",
    "    \n",
    "    return metrics, prompts, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f4ae84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "19\n",
      "29\n",
      "39\n",
      "49\n",
      "59\n",
      "69\n",
      "79\n",
      "89\n",
      "99\n",
      "109\n",
      "119\n",
      "129\n",
      "139\n",
      "149\n",
      "159\n",
      "169\n",
      "179\n",
      "189\n",
      "199\n",
      "209\n",
      "219\n",
      "229\n",
      "239\n",
      "249\n",
      "259\n",
      "269\n",
      "279\n",
      "289\n",
      "299\n",
      "309\n",
      "319\n",
      "329\n",
      "339\n",
      "349\n",
      "359\n",
      "369\n",
      "379\n",
      "389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics:\n",
      "{'mean_f1': 0.11844365862181162, 'mean_bertscore_f1': 0.018581148236989975, 'exact_match_rate': 0.0, 'avg_pred_length_words': 90.31979695431473, 'num_examples': 394}\n"
     ]
    }
   ],
   "source": [
    "metrics, prompts, responses = evaluate_rag_model(df_eval, top_k=5)\n",
    "print(\"Final evaluation metrics:\")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
