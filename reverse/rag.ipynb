{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c65f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e12989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize embedding and language models\n",
    "embedding_model = 'snowflake-arctic-embed'\n",
    "language_model = 'llama3.1:8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d86d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_excel('../combined_slang_data_multi.xlsx')\n",
    "\n",
    "# create a string per unique word by combining all rows for that word\n",
    "combined_rows_as_strings_dict = {}\n",
    "for i, row in df.iterrows():\n",
    "    # append or create string entry for each word\n",
    "    word = row['slang']\n",
    "    row_string = \"\\n\".join(f\"{k}: {v}\" for k, v in row.items())\n",
    "    combined_rows_as_strings_dict[word] = combined_rows_as_strings_dict.get(word, \"\") + \"\\n\" + row_string\n",
    "combined_rows_as_strings = list(combined_rows_as_strings_dict.values())\n",
    "\n",
    "VECTOR_DB = [] # a list of (text, embedding) tuples\n",
    "\n",
    "def add_text_to_db(text):\n",
    "    embedding = ollama.embed(model=embedding_model, input=text)['embeddings'][0]\n",
    "    VECTOR_DB.append((text, embedding))\n",
    "\n",
    "for row_text in combined_rows_as_strings:\n",
    "    add_text_to_db(row_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a5cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv('eval_reverse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a16a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    # simple tokenizer; you can make this smarter if you want\n",
    "    return text.lower().split()\n",
    "\n",
    "# texts in the same order as VECTOR_DB\n",
    "doc_texts = [text for text, _ in VECTOR_DB]\n",
    "tokenized_docs = [tokenize(t) for t in doc_texts]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a7e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MATRIX = np.vstack([np.asarray(emb, dtype=np.float32) for _, emb in VECTOR_DB])\n",
    "EMB_NORMS = np.linalg.norm(EMB_MATRIX, axis=1) + 1e-9\n",
    "DOC_TEXTS = [text for text, _ in VECTOR_DB]  # keep alignment\n",
    "\n",
    "def hybrid_search(query_text, top_k=10, alpha=0.4):\n",
    "    q_embedding = np.asarray(\n",
    "        ollama.embed(model=embedding_model, input=query_text)['embeddings'][0],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    q_norm = np.linalg.norm(q_embedding) + 1e-9\n",
    "\n",
    "    # vectorized dense scores\n",
    "    dense_scores = (EMB_MATRIX @ q_embedding) / (EMB_NORMS * q_norm)\n",
    "\n",
    "    # BM25\n",
    "    query_tokens = tokenize(query_text)\n",
    "    bm25_scores = np.array(bm25.get_scores(query_tokens), dtype=np.float32)\n",
    "\n",
    "    # min-max normalize\n",
    "    def minmax_normalize(x):\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        if x_max == x_min:\n",
    "            return np.zeros_like(x)\n",
    "        return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    dense_norm = minmax_normalize(dense_scores)\n",
    "    bm25_norm = minmax_normalize(bm25_scores)\n",
    "\n",
    "    hybrid_scores = alpha * dense_norm + (1 - alpha) * bm25_norm\n",
    "\n",
    "    # fast top-k with argpartition\n",
    "    top_idx = np.argpartition(-hybrid_scores, top_k - 1)[:top_k]\n",
    "    # sort those top-k\n",
    "    top_idx = top_idx[np.argsort(-hybrid_scores[top_idx])]\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(top_idx, start=1):\n",
    "        text = DOC_TEXTS[idx]\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"index\": int(idx),\n",
    "            \"hybrid_score\": float(hybrid_scores[idx]),\n",
    "            \"dense_score\": float(dense_scores[idx]),\n",
    "            \"bm25_score\": float(bm25_scores[idx]),\n",
    "            \"text\": text,\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39d82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_prompt(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt in the SAME style as your finetune prompts,\n",
    "    but for the reverse task: sentence with slang -> choose meaning.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You will be given a sentence that contains a modern slang term and four possible interpretations.\n",
    "Choose the option that best explains the meaning of the sentence in standard English.\n",
    "\n",
    "Sentence: \"{row['sentence']}\"\n",
    "\n",
    "Options:\n",
    "A) {row['option_A']}\n",
    "B) {row['option_B']}\n",
    "C) {row['option_C']}\n",
    "D) {row['option_D']}\n",
    "\n",
    "Answer with just the letter.\"\"\"\n",
    "\n",
    "def build_rag_prompt(pre_rag_prompt, context):\n",
    "    prompt = f\"\\nContext:\\n{context}\\n{pre_rag_prompt}\"\n",
    "    return prompt\n",
    "\n",
    "def build_rag_context(prompt, top_k=5):\n",
    "    results = hybrid_search(prompt, top_k=top_k)\n",
    "    context = \"\\n\".join([r['text'] for r in results])\n",
    "    return context\n",
    "\n",
    "def evaluate_rag_model(df_eval, top_k=5):\n",
    "    prompts = []\n",
    "    responses = []\n",
    "    num_correct = 0\n",
    "    num_questions = len(df_eval)\n",
    "    for i in range(len(df_eval)):\n",
    "        row = df_eval.iloc[i]\n",
    "        pre_rag_prompt = row_to_prompt(row)\n",
    "        context = build_rag_context(pre_rag_prompt, top_k=top_k)\n",
    "\n",
    "        rag_prompt = build_rag_prompt(pre_rag_prompt, context)\n",
    "        response = ollama.chat(\n",
    "            model=language_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers multiple choice questions with just the correct letter.\"},\n",
    "                {\"role\": \"user\", \"content\": rag_prompt}, # <-- change this line back to `rag_prompt` to use the context\n",
    "            ]\n",
    "        )\n",
    "        answer = response.message.content[0]\n",
    "        correct_answer = df_eval.iloc[i]['correct']\n",
    "        if answer.lower() == correct_answer.lower():\n",
    "            num_correct += 1\n",
    "\n",
    "        # logging\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i + 1} questions, accuracy: {num_correct / (i + 1):.2%}\")\n",
    "\n",
    "        prompts.append(rag_prompt)\n",
    "        responses.append(answer)\n",
    "\n",
    "    accuracy = num_correct / num_questions\n",
    "    return accuracy, prompts, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560e5838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 questions, accuracy: 100.00%\n",
      "Processed 11 questions, accuracy: 100.00%\n",
      "Processed 21 questions, accuracy: 100.00%\n",
      "Processed 31 questions, accuracy: 100.00%\n",
      "Processed 41 questions, accuracy: 100.00%\n",
      "Processed 51 questions, accuracy: 100.00%\n",
      "Processed 61 questions, accuracy: 100.00%\n",
      "Processed 71 questions, accuracy: 100.00%\n",
      "Processed 81 questions, accuracy: 100.00%\n",
      "Processed 91 questions, accuracy: 100.00%\n",
      "Processed 101 questions, accuracy: 100.00%\n",
      "Processed 111 questions, accuracy: 100.00%\n",
      "Processed 121 questions, accuracy: 100.00%\n",
      "Processed 131 questions, accuracy: 100.00%\n",
      "Processed 141 questions, accuracy: 100.00%\n",
      "Processed 151 questions, accuracy: 100.00%\n",
      "Processed 161 questions, accuracy: 100.00%\n",
      "Processed 171 questions, accuracy: 99.42%\n",
      "Processed 181 questions, accuracy: 98.90%\n",
      "Processed 191 questions, accuracy: 97.91%\n",
      "Processed 201 questions, accuracy: 98.01%\n",
      "Processed 211 questions, accuracy: 98.10%\n",
      "Processed 221 questions, accuracy: 97.74%\n",
      "Processed 231 questions, accuracy: 97.84%\n",
      "Processed 241 questions, accuracy: 97.93%\n",
      "Processed 251 questions, accuracy: 98.01%\n",
      "Processed 261 questions, accuracy: 98.08%\n",
      "Processed 271 questions, accuracy: 98.15%\n",
      "Processed 281 questions, accuracy: 98.22%\n",
      "Processed 291 questions, accuracy: 98.28%\n",
      "Processed 301 questions, accuracy: 98.34%\n",
      "Processed 311 questions, accuracy: 98.39%\n",
      "Processed 321 questions, accuracy: 98.44%\n",
      "Processed 331 questions, accuracy: 98.19%\n",
      "Processed 341 questions, accuracy: 98.24%\n",
      "Processed 351 questions, accuracy: 98.29%\n",
      "Processed 361 questions, accuracy: 98.34%\n",
      "Processed 371 questions, accuracy: 98.11%\n",
      "Processed 381 questions, accuracy: 97.90%\n",
      "Processed 391 questions, accuracy: 97.95%\n",
      "Final accuracy: 97.97%\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy, prompts, responses = evaluate_rag_model(df_eval, top_k=5)\n",
    "print(f\"Final accuracy: {eval_accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
