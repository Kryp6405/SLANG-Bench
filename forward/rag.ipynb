{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8c17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fc4e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 547 entries\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_excel('../combined_slang_data_multi.xlsx')\n",
    "print(f'Loaded {len(df)} entries')\n",
    "\n",
    "# initialize embedding and language models\n",
    "embedding_model = 'snowflake-arctic-embed'\n",
    "language_model = 'llama3.1:8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a987df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv('eval_mcq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd8c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a string per unique word by combining all rows for that word\n",
    "combined_rows_as_strings_dict = {}\n",
    "for i, row in df.iterrows():\n",
    "    # append or create string entry for each word\n",
    "    word = row['slang']\n",
    "    row_string = \"\\n\".join(f\"{k}: {v}\" for k, v in row.items())\n",
    "    combined_rows_as_strings_dict[word] = combined_rows_as_strings_dict.get(word, \"\") + \"\\n\" + row_string\n",
    "combined_rows_as_strings = list(combined_rows_as_strings_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84cd30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB = [] # a list of (text, embedding) tuples\n",
    "\n",
    "def add_text_to_db(text):\n",
    "    embedding = ollama.embed(model=embedding_model, input=text)['embeddings'][0]\n",
    "    VECTOR_DB.append((text, embedding))\n",
    "\n",
    "for row_text in combined_rows_as_strings:\n",
    "    add_text_to_db(row_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903362df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    # simple tokenizer; you can make this smarter if you want\n",
    "    return text.lower().split()\n",
    "\n",
    "# texts in the same order as VECTOR_DB\n",
    "doc_texts = [text for text, _ in VECTOR_DB]\n",
    "tokenized_docs = [tokenize(t) for t in doc_texts]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00e72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MATRIX = np.vstack([np.asarray(emb, dtype=np.float32) for _, emb in VECTOR_DB])\n",
    "EMB_NORMS = np.linalg.norm(EMB_MATRIX, axis=1) + 1e-9\n",
    "DOC_TEXTS = [text for text, _ in VECTOR_DB]  # keep alignment\n",
    "\n",
    "def hybrid_search(query_text, top_k=10, alpha=0.4):\n",
    "    q_embedding = np.asarray(\n",
    "        ollama.embed(model=embedding_model, input=query_text)['embeddings'][0],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    q_norm = np.linalg.norm(q_embedding) + 1e-9\n",
    "\n",
    "    # vectorized dense scores\n",
    "    dense_scores = (EMB_MATRIX @ q_embedding) / (EMB_NORMS * q_norm)\n",
    "\n",
    "    # BM25\n",
    "    query_tokens = tokenize(query_text)\n",
    "    bm25_scores = np.array(bm25.get_scores(query_tokens), dtype=np.float32)\n",
    "\n",
    "    # min-max normalize\n",
    "    def minmax_normalize(x):\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        if x_max == x_min:\n",
    "            return np.zeros_like(x)\n",
    "        return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    dense_norm = minmax_normalize(dense_scores)\n",
    "    bm25_norm = minmax_normalize(bm25_scores)\n",
    "\n",
    "    hybrid_scores = alpha * dense_norm + (1 - alpha) * bm25_norm\n",
    "\n",
    "    # fast top-k with argpartition\n",
    "    top_idx = np.argpartition(-hybrid_scores, top_k - 1)[:top_k]\n",
    "    # sort those top-k\n",
    "    top_idx = top_idx[np.argsort(-hybrid_scores[top_idx])]\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(top_idx, start=1):\n",
    "        text = DOC_TEXTS[idx]\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"index\": int(idx),\n",
    "            \"hybrid_score\": float(hybrid_scores[idx]),\n",
    "            \"dense_score\": float(dense_scores[idx]),\n",
    "            \"bm25_score\": float(bm25_scores[idx]),\n",
    "            \"text\": text,\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af7e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, options):\n",
    "    options_text = \"\\n\".join([f\"{chr(97 + i)}) {opt}\" for i, opt in enumerate(options)])\n",
    "    prompt = f\"Question: {question}\\nOptions:\\n{options_text}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "prompt = build_prompt(\n",
    "    df_eval.iloc[0]['sentence'],\n",
    "    [df_eval.iloc[0][opt] for opt in ['option_a', 'option_b', 'option_c', 'option_d']]\n",
    ")\n",
    "\n",
    "# test model response using language_model\n",
    "response = ollama.chat(\n",
    "    model=language_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers multiple choice questions with just the correct letter.\"},\n",
    "        {\"role\": \"user\", \"content\": 'the sun is a star. A) true, B) false'},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560efeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with RAG\n",
    "\n",
    "def build_rag_prompt(question, options, context):\n",
    "    options_text = \"\\n\".join([f\"{chr(97 + i)}) {opt}\" for i, opt in enumerate(options)])\n",
    "    prompt = f\"\\nContext:\\n{context}\\nQuestion: {question}\\nOptions:\\n{options_text}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "def build_rag_context(prompt, top_k=5):\n",
    "    results = hybrid_search(prompt, top_k=top_k)\n",
    "    context = \"\\n\".join([r['text'] for r in results])\n",
    "    return context\n",
    "\n",
    "def evaluate_rag_model(df_eval, top_k=5):\n",
    "    prompts = []\n",
    "    responses = []\n",
    "    num_correct = 0\n",
    "    num_questions = len(df_eval)\n",
    "    for i in range(len(df_eval)):\n",
    "        question = df_eval.iloc[i]['sentence']\n",
    "        options = [df_eval.iloc[i][opt] for opt in ['option_a', 'option_b', 'option_c', 'option_d']]\n",
    "\n",
    "        pre_rag_prompt = build_prompt(question, options)\n",
    "        context = build_rag_context(pre_rag_prompt, top_k=top_k)\n",
    "\n",
    "        rag_prompt = build_rag_prompt(question, options, context)\n",
    "        response = ollama.chat(\n",
    "            model=language_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers multiple choice questions with just the correct letter.\"},\n",
    "                {\"role\": \"user\", \"content\": rag_prompt},\n",
    "            ]\n",
    "        )\n",
    "        answer = response.message.content[0]\n",
    "        correct_answer = df_eval.iloc[i]['correct_answer']\n",
    "        if answer.lower() == correct_answer.lower():\n",
    "            num_correct += 1\n",
    "\n",
    "        # logging\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i + 1} questions, accuracy: {num_correct / (i + 1):.2%}\")\n",
    "\n",
    "        prompts.append(rag_prompt)\n",
    "        responses.append(answer)\n",
    "\n",
    "    accuracy = num_correct / num_questions\n",
    "    return accuracy, prompts, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d0cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 questions, accuracy: 100.00%\n",
      "Processed 11 questions, accuracy: 90.91%\n",
      "Processed 21 questions, accuracy: 90.48%\n",
      "Processed 31 questions, accuracy: 83.87%\n",
      "Processed 41 questions, accuracy: 85.37%\n",
      "Processed 51 questions, accuracy: 84.31%\n",
      "Processed 61 questions, accuracy: 86.89%\n",
      "Processed 71 questions, accuracy: 85.92%\n",
      "Processed 81 questions, accuracy: 85.19%\n",
      "Processed 91 questions, accuracy: 86.81%\n",
      "Processed 101 questions, accuracy: 88.12%\n",
      "Processed 111 questions, accuracy: 88.29%\n",
      "Processed 121 questions, accuracy: 89.26%\n",
      "Processed 131 questions, accuracy: 87.79%\n",
      "Processed 141 questions, accuracy: 86.52%\n",
      "Processed 151 questions, accuracy: 86.09%\n",
      "Processed 161 questions, accuracy: 86.34%\n",
      "Processed 171 questions, accuracy: 86.55%\n",
      "Processed 181 questions, accuracy: 86.19%\n",
      "Processed 191 questions, accuracy: 85.34%\n",
      "Processed 201 questions, accuracy: 85.07%\n",
      "Processed 211 questions, accuracy: 85.78%\n",
      "Processed 221 questions, accuracy: 85.97%\n",
      "Processed 231 questions, accuracy: 84.85%\n",
      "Processed 241 questions, accuracy: 84.65%\n",
      "Processed 251 questions, accuracy: 84.86%\n",
      "Processed 261 questions, accuracy: 84.67%\n",
      "Processed 271 questions, accuracy: 83.76%\n",
      "Processed 281 questions, accuracy: 82.56%\n",
      "Processed 291 questions, accuracy: 82.82%\n",
      "Processed 301 questions, accuracy: 82.72%\n",
      "Processed 311 questions, accuracy: 81.67%\n",
      "Processed 321 questions, accuracy: 81.62%\n",
      "Processed 331 questions, accuracy: 81.57%\n",
      "Processed 341 questions, accuracy: 81.82%\n",
      "Processed 351 questions, accuracy: 81.48%\n",
      "Processed 361 questions, accuracy: 80.61%\n",
      "Processed 371 questions, accuracy: 80.05%\n",
      "Processed 381 questions, accuracy: 79.27%\n",
      "Processed 391 questions, accuracy: 78.77%\n",
      "Processed 401 questions, accuracy: 79.30%\n",
      "Processed 411 questions, accuracy: 79.81%\n",
      "Processed 421 questions, accuracy: 79.57%\n",
      "Processed 431 questions, accuracy: 79.81%\n",
      "Processed 441 questions, accuracy: 79.82%\n",
      "Processed 451 questions, accuracy: 79.82%\n",
      "Processed 461 questions, accuracy: 79.83%\n",
      "Processed 471 questions, accuracy: 79.19%\n",
      "Processed 481 questions, accuracy: 79.21%\n",
      "Processed 491 questions, accuracy: 79.02%\n",
      "Processed 501 questions, accuracy: 78.64%\n",
      "Processed 511 questions, accuracy: 78.86%\n",
      "Processed 521 questions, accuracy: 78.31%\n",
      "Processed 531 questions, accuracy: 78.34%\n",
      "Processed 541 questions, accuracy: 78.37%\n",
      "Processed 551 questions, accuracy: 77.86%\n",
      "Processed 561 questions, accuracy: 78.07%\n",
      "Processed 571 questions, accuracy: 78.28%\n",
      "Processed 581 questions, accuracy: 77.80%\n",
      "Processed 591 questions, accuracy: 77.33%\n",
      "Processed 601 questions, accuracy: 76.71%\n",
      "Processed 611 questions, accuracy: 76.43%\n",
      "Processed 621 questions, accuracy: 76.33%\n",
      "Processed 631 questions, accuracy: 76.39%\n",
      "Processed 641 questions, accuracy: 75.98%\n",
      "Processed 651 questions, accuracy: 76.04%\n",
      "Processed 661 questions, accuracy: 75.64%\n",
      "Processed 671 questions, accuracy: 75.56%\n",
      "Processed 681 questions, accuracy: 75.92%\n",
      "RAG Model Accuracy: 75.94%\n"
     ]
    }
   ],
   "source": [
    "# actually test the RAG model\n",
    "\n",
    "accuracy_rag, prompts, responses = evaluate_rag_model(df_eval, top_k=5)\n",
    "print(f\"RAG Model Accuracy: {accuracy_rag:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
